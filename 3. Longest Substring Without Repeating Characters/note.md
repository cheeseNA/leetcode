# step1

time: 36:36

naive な方法では O(N^3).
しゃくとり法？ちがうか.

各文字に対して各位置でのそれまでの出現回数を持っておけば, 26 回で妥当かチェックできる, よって O(N^2)に削減可能.

始まりの位置を固定したときに, 終わりの位置は重複が生じるまでできるだけ伸ばせばそれが最長のものとなる. それならば累積和ではなく set をもつことで管理可能だ. ある始まりの位置に対して最善の終わりの位置がわかったら, 次は始まりの位置を重複が解消されるまで前にずらしていく. 解消されたら今度は後ろを伸ばす. これで time complexity は O(N), space complexity は O(1).

実装では, end_index が終端のときの処理がややこしい…
また, start_index, end_index の位置がシュミレーションしにくい. 思ったように動かず print デバッグをしてしまった.
while の内部にしか return が無いのが気になる. また, while True をまた使用してしまっている. しゃくとり法の実装に慣れたい.

# step2

見た実装

- https://discord.com/channels/1084280443945353267/1199984201521430588/1200190786357186580
  - seen って 256 要素でいいの？ASCII を前提にしているとの仮定の下？それにしても hashmap で管理したほうが良い気がする.
  - appeared かどうかのみを持っておくのではなく, その位置を持っておくことで, start を一回の処理で更新できるようにしているのが良い. しゃくとり法よりもわかりやすい実装となっている.
- google docs
  - しゃくとり法のような実装になっているが, 読みやすい.
  - discard と remove の違いは, discard はなかったら無視するが, remove はなかったら KeyError を raise する.
    - 実装はhttps://github.com/python/cpython/blob/649857a1574a02235ccfac9e2ac1c12914cf8fe0/Objects/setobject.c#L347 にありますね.
- https://discord.com/channels/1084280443945353267/1200089668901937312/1215986234812403752
  - while じゃなくて for でも良い気がする.
- https://discord.com/channels/1084280443945353267/1196472827457589338/1196472926862577745
  - step3 の実装がしっくりくる.

自分は while True の中で right も left も動かすような実装になっているため, 読みにくく煩雑なコードになっていた. left_index を固定した場合…という考え方で解き始めていたためこの様になってしまっていた.
また, len(s) == 0 のときの早期脱出は必要ない.
